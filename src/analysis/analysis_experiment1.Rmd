---
title: "Simulation of implied shape: Experiment 1"
# author: "Sean Trott"
date: "10/28/2020"
output:
  html_document:
    toc: yes
    toc_float: yes
    # code_folding: hide
  pdf_document: default
  word_document:
    toc: yes
---

```{r include=FALSE}
library(tidyverse)
library(lme4)
```



# Load data

```{r}
# setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")

## First, load data about individual items
df_items = read_csv("../../data/items.csv")
nrow(df_items)

df_items_gathered = df_items %>%
  pivot_longer(c(list_1, list_2, list_3, list_4), 
               names_to = "list", values_to = "version")
nrow(df_items_gathered)



## Now, load actual experimental data

df_e1 = read_csv("../../data/processed/translation_e1_processed.csv")
length(unique(df_e1$subject))

# Double-check experimental data
# There should be 120 rows for both tasks combined for each subject, so divide amount per subject by 120
table(df_e1$task) / 120

```

## Demographic information

```{r}

# Load subject-wise data
df_subjects = read_csv("../../data/processed/subjects.csv")
nrow(df_subjects)

table(df_subjects$gender)
table(df_subjects$native_speaker)

# One person wrote "test" for age answer
df_subjects$age_numeric = as.numeric(df_subjects$age)
mean(df_subjects$age_numeric, na.rm = TRUE)
sd(df_subjects$age_numeric, na.rm = TRUE)
range(df_subjects$age_numeric, na.rm = TRUE)
```


## Preprocess and merge data

Set response keys for experimental data.

```{r}
df_e1$response = fct_recode(
  factor(df_e1$key_press),
  "Yes" = "89",
  "No" = "78"
)

```

Indicate the correct response, based on whether item was a filler ("No") or not ("Yes").

```{r}
df_e1 = df_e1 %>%
  mutate(correct_response = case_when(
    type == "filler" ~ "No",
    is.na(type) ~ "Yes"
  )) %>%
  mutate(correct = correct_response == response)
```


```{r}
df_ppt_accuracy_plaus = df_e1 %>%
  # filter(type == "filler") %>%  ### Do we want to filter to fillers for the plausibility sentences?
  filter(task == "plausibility") %>%
  group_by(subject) %>%
  summarise(plaus_accuracy = mean(correct))

df_ppt_accuracy_plaus %>%
  ggplot(aes(x = plaus_accuracy)) +
  geom_histogram() +
  labs(x = "Accuracy on all plausibility items") +
  theme_minimal()

df_ppt_accuracy_recall = df_e1 %>%
  filter(type == "filler") %>%  
  filter(task == "recall") %>%
  group_by(subject) %>%
  summarise(recall_accuracy = mean(correct))

df_ppt_accuracy_recall %>%
  ggplot(aes(x = recall_accuracy)) +
  geom_histogram() +
  labs(x = "Accuracy on filler recall items") +
  theme_minimal()

```

## Merge with item-level information

Now merge with items, and label for whether the picture shape matches the sentence shape.


```{r}
df_merged = df_e1 %>%
  left_join(df_items_gathered, on = c(list, object))

## Label whether or not shape/sentence is listed as matching
df_merged = df_merged %>%
  mutate(
    match = case_when(
      type == "filler" ~ "filler",
      version %in% c('aa', 'bb') ~ "yes",
      (version %in% c('aa', 'bb')) == FALSE ~ "no"
    )
  )

df_merged = df_merged %>%
  left_join(df_ppt_accuracy_plaus, on = c(subject)) %>%
  left_join(df_ppt_accuracy_recall, on = c(subject))

nrow(df_merged)

```


# Analyses

## Does matching orientation predict accuracy on recall items?

```{r}
df_merged_recall_critical = df_merged %>%
  filter(task == "recall") %>%
  filter(match != "filler")


df_merged_recall_critical %>%
  group_by(match) %>%
  summarise(accuracy = mean(correct))


df_merged_recall_critical %>%
  mutate(correct_numeric = as.numeric(correct)) %>%
  ggplot(aes(x = match,
             y = correct_numeric)) +
  stat_summary (fun = function(x){mean(x)},
                fun.min = function(x){mean(x) - 2*sd(x)/sqrt(length(x))},
                fun.max = function(x){mean(x) + 2*sd(x)/sqrt(length(x))},
                geom= 'pointrange', 
                position=position_dodge(width=0.95)) +
  scale_y_continuous(limits = c(0, 1)) +
  geom_hline(yintercept = .5, linetype = "dotted") +
  labs(x = "Matching shape",
       y = "Accuracy") +
  theme_minimal()

model_full = glmer(data = df_merged_recall_critical,
                   correct ~ match + 
                     (1 + match | subject) +
                     (1 + match | object),
                   family = binomial())

model_reduced = glmer(data = df_merged_recall_critical,
                   correct ~ 
                     (1 + match | subject) +
                     (1 + match | object),
                   family = binomial())

summ = summary(model_full)
summ
anova(model_full, model_reduced)

```


This model estimates that the probability of a correct answer in the `match` condition (chosen as the reference class, i.e., the `Intercept`) is `r round(exp(summ$coefficients[1])/(1+ exp(summ$coefficients[1])), 2)`, or `r scales::percent(round(exp(summ$coefficients[1])/(1+ exp(summ$coefficients[1])), 4))`.

The coefficient for the `match` condition is `r round(summ$coefficients[2], 4)`. This means the **odds** of success in the `match` condition are approximately `r round(exp(summ$coefficients[2]), 2)` as large as in the `mismatch` condition. (*TO DO*: Check this.)

We can also visualize the model outputs in terms of raw probabilities:

```{r}

df_merged_recall_critical$pred = predict(model_full, type = "response")

df_merged_recall_critical %>%
  ggplot(aes(x = pred,
             fill = match)) +
  geom_density(alpha = .6) +
  labs(x = "P(correct)") +
  theme_minimal()
```


## Does participant-level d-prime vary for match vs. mismatch orientation?

The above analysis does not account for **false alarm rates**, i.e., for the *filler* recall trials, whereas the original paper [Pecher et al (2009)](https://journals.sagepub.com/doi/pdf/10.1080/17470210802633255) does.

Here, we replicate this analysis by calculating `d'` (or **d-prime**) for each participant for the `match` and `mismatch` conditions.

### Calculating d'

D-prime is defined as:

$d = z(H) - z(FA)$

We want to calculate this for both the `match` and `mismatch` conditions. To do this, we calculate:

- hit rate: accuracy on the critical recall items (correct answer is always yes)  
- false alarm rate: $1 - accuracy$ on the filler recall items (correct answer is always no) 

```{r}
df_recall = df_merged %>%
  filter(task == "recall") %>%
  filter(match != "filler")


df_dprime = df_recall %>%
  group_by(subject, match) %>%
  ### Hit rate: p(answer = yes | correct = yes), so accuracy for critical items
  ### False alarm rate: p(answer = yes | correct = no), so 1 - accuracy for filler items
  summarise(hit_rate = mean(correct),
            # n_h = sum(correct),
            # n_m = n() - sum(correct),
            # filler_accuracy = mean(recall_accuracy),
            # n_cr = mean(recall_accuracy) * n(),
            false_alarm_rate = 1 - mean(recall_accuracy)) %>%
  ## Adjust hit rate and false alarm rate for 0% and 100% observations
  mutate(hit_rate = 
           case_when(
             hit_rate == 1 ~ (30 - .5)/30,
             hit_rate == 0 ~ .5/30,
             hit_rate != 0 & hit_rate != 1 ~ hit_rate
           ),
         false_alarm_rate = 
           case_when(
             false_alarm_rate == 1 ~ (30 - .5)/30,
             false_alarm_rate == 0 ~ .5/30,
             false_alarm_rate != 0 & false_alarm_rate != 1 ~ false_alarm_rate
           )
  ) %>%
  ### Calculate d_prime
  mutate(d_prime = qnorm(hit_rate) - qnorm(false_alarm_rate))
nrow(df_dprime) / 2

df_dprime %>%
  ggplot(aes(x = d_prime)) +
  geom_histogram() +
  labs(x = "d'") +
  theme_minimal()

## Remove subjects with d' â‰¤ 0, as in Pecher et al (2009)
df_dprime_avg = df_dprime %>%
  group_by(subject) %>%
  summarise(d_mean = mean(d_prime))

df_dprime = df_dprime %>%
  left_join(df_dprime_avg, on = "subject") %>%
  filter(d_mean > 0)
nrow(df_dprime) / 2

### Summary stats
df_dprime %>%
  # filter(d_prime != Inf) %>%
  group_by(match) %>%
  summarise(mean_dprime = mean(d_prime),
            sd_dprime = sd(d_prime))

```


### Model comparison

Now, we as whether `d'` differs by `match` condition`.

```{r}

model_full = lmer(data = df_dprime,
                  d_prime ~ match + (1 | subject),
                  REML = FALSE)

model_reduced = lmer(data = df_dprime,
                  d_prime ~ (1 | subject),
                  REML = FALSE)

anova(model_full, model_reduced)
summary(model_full)
```

We can also visualize this difference:

```{r}
df_dprime %>%
  ggplot(aes(x = match,
             y = d_prime)) +
  geom_bar(stat = "summary", fun = "mean") +
  stat_summary (fun = function(x){mean(x)},
                fun.min = function(x){mean(x) - 2*sd(x)/sqrt(length(x))},
                fun.max = function(x){mean(x) + 2*sd(x)/sqrt(length(x))},
                geom= 'pointrange', 
                position=position_dodge(width=0.95)) +
  # scale_y_continuous(limits = c(-.1, 3)) +
  labs(x = "Matching shape",
       y = "d'") +
  theme_minimal()


df_dprime %>%
  ggplot(aes(x = d_prime,
             fill = match)) +
  labs(x = "d'") +
  geom_density(alpha = .6) +
  theme_minimal()
```

These error bars are pretty big. But in fact, there's a lot of by-subject variation. So if we instead visualize the **residuals** from `model_reduced`, we get a better picture:

```{r}
df_dprime$resid = residuals(model_reduced)

df_dprime %>%
  ggplot(aes(x = match,
             y = resid)) +
  stat_summary (fun = function(x){mean(x)},
                fun.min = function(x){mean(x) - 1*sd(x)/sqrt(length(x))},
                fun.max = function(x){mean(x) + 1*sd(x)/sqrt(length(x))},
                geom= 'pointrange', 
                position=position_dodge(width=0.95)) +
  labs(x = "Matching shape",
       y = "Residuals from subjects-only intercepts") +
  theme_minimal()


df_dprime %>%
  ggplot(aes(x = resid,
             fill = match)) +
  geom_density(alpha = .6) +
  labs(x = "Residuals from subjects-only intercepts") +
  theme_minimal()
```



# Exploratory analysis 1: Non-native speakers

We had a fair number of non-native speakers of English in this study. It would be useful to know whether the effect holds up for the non-native population specifically.

## Accuracy effect 

First, let's compare the accuracy effect:

```{r}
df_merged_recall_critical %>%
  mutate(correct_numeric = as.numeric(correct)) %>%
  ggplot(aes(x = match,
             y = correct_numeric)) +
  stat_summary (fun = function(x){mean(x)},
                fun.min = function(x){mean(x) - 2*sd(x)/sqrt(length(x))},
                fun.max = function(x){mean(x) + 2*sd(x)/sqrt(length(x))},
                geom= 'pointrange', 
                position=position_dodge(width=0.95)) +
  scale_y_continuous(limits = c(0, 1)) +
  geom_hline(yintercept = .5, linetype = "dotted") +
  labs(x = "Matching shape",
       y = "Accuracy") +
  theme_minimal() +
  facet_grid(~native_speaker)

```


Visually, it seems like there's no effect among the non-native speakers. We can test this in two ways: first, by testing for an interaction of `match` by `native_speaker`; and second, by testing for the presence / strength of the effect in each sub-population.

### Interaction analysis

There is indeed a significant (though just barely) interaction of `match` by `native_speaker`, in the expected direction. (I.e., the probability of a correct response is larger for `matching` trials for `native speakers`.)

```{r}

model_full = glmer(data = df_merged_recall_critical,
                   correct ~ match * native_speaker + 
                     (1 + match | subject) +
                     (1 + match | object),
                   family = binomial())

model_reduced = glmer(data = df_merged_recall_critical,
                   correct ~ match + native_speaker +
                     (1 + match | subject) +
                     (1 + match | object),
                   family = binomial())

summary(model_full)
anova(model_full, model_reduced)

```



### Subset analysis

Now let's compare the sub-populations, first descriptively:

```{r}
df_merged_recall_critical %>%
  group_by(native_speaker, match) %>%
  summarise(accuracy = mean(correct))
```


Now we do inferential tests:

```{r}
model_full = glmer(data = filter(df_merged_recall_critical, native_speaker == "Y"),
                   correct ~ match + 
                     (1 + match | subject) +
                     (1 + match | object),
                   family = binomial())

model_reduced = glmer(data = filter(df_merged_recall_critical, native_speaker == "Y"),
                   correct ~ 
                     (1 + match | subject) +
                     (1 + match | object),
                   family = binomial())

summary(model_full)
anova(model_full, model_reduced)
```

There's a very clear effect, again consistent with the effect reported earlier: correct responses are more likely for matching trials.

Whereas in the non-native speaker group, a model including `match` does not improve model fit.

```{r}
model_full = glmer(data = filter(df_merged_recall_critical, native_speaker == "N"),
                   correct ~ match + 
                     (1 + match | subject) +
                     (1 + match | object),
                   family = binomial())

model_reduced = glmer(data = filter(df_merged_recall_critical, native_speaker == "N"),
                   correct ~ 
                     (1 + match | subject) +
                     (1 + match | object),
                   family = binomial())

summary(model_full)
anova(model_full, model_reduced)
```


## Comparing d'

The accuracy analysis does not control for false-positive rates on the filler recall items. Here, we also extend the d-prime analysis from above and split it out by native and non-native speaker.

First, we merge the data:

```{r}
df_dprime = df_dprime %>%
  left_join(df_subjects, on = "subject")
nrow(df_dprime) / 2
```

Now let's do a quick visualization:

```{r}
df_dprime %>%
  ggplot(aes(x = match,
             y = d_prime)) +
  geom_bar(stat = "summary", fun = "mean") +
  stat_summary (fun = function(x){mean(x)},
                fun.min = function(x){mean(x) - 1*sd(x)/sqrt(length(x))},
                fun.max = function(x){mean(x) + 1*sd(x)/sqrt(length(x))},
                geom= 'pointrange', 
                position=position_dodge(width=0.95)) +
  # scale_y_continuous(limits = c(-.1, 3)) +
  labs(x = "Matching shape",
       y = "d'") +
  theme_minimal() +
  facet_grid(~native_speaker)

df_dprime %>%
  ggplot(aes(x = d_prime,
             fill = match)) +
  labs(x = "d'") +
  geom_density(alpha = .6) +
  theme_minimal() +
  facet_grid(~native_speaker)

```


### Interaction analysis

The interaction between `match` and `native_speaker` is not quite significant, though it is close. Additionally, the coefficient points in the right direction: `d'` is higher for `match` trials for `native speakers` specifically.

```{r}

model_full = lmer(data = df_dprime,
                  d_prime ~ match * native_speaker + (1 | subject),
                  REML = FALSE)

model_reduced = lmer(data = df_dprime,
                  d_prime ~  match + native_speaker + (1 | subject),
                  REML = FALSE)

anova(model_full, model_reduced)
summary(model_full)

```



### Subset analysis

Now let's compare the sub-populations, first descriptively:

```{r}
df_dprime %>%
  group_by(native_speaker, match) %>%
  summarise(mean_dprime = mean(d_prime))
```


Now we do inferential tests:

```{r}
model_full = lmer(data = filter(df_dprime, native_speaker == "Y"),
                  d_prime ~ match + (1 | subject),
                  REML = FALSE)

model_reduced = lmer(data = filter(df_dprime, native_speaker == "Y"),
                  d_prime ~  (1 | subject),
                  REML = FALSE)

anova(model_full, model_reduced)
summary(model_full)
```

There's a very clear effect, again consistent with the effect reported earlier: d-prime is higher for matching trials.

Whereas in the non-native speaker group, a model including `match` does not improve model fit.

```{r}
model_full = lmer(data = filter(df_dprime, native_speaker == "N"),
                  d_prime ~ match + (1 | subject),
                  REML = FALSE)

model_reduced = lmer(data = filter(df_dprime, native_speaker == "N"),
                  d_prime ~  (1 | subject),
                  REML = FALSE)

anova(model_full, model_reduced)
summary(model_full)
```


## Brief discussion

We analyzed the effect(s) among native and non-native speakers:

```{r}
table(df_subjects$native_speaker)
table(df_dprime$native_speaker) / 2
```

There is a robust effect of `match` among participants who self-report as native speakers of English. Matching trials are more likely to be answered correctly, and are associated with higher d-prime values.

In contrast, there is no detectable effect of `match` among participants who self-report as non-native speakers of English. There is also a (barely) significant interaction between `match` and `native speaker` in predicting `correct response`, and a trending significant interaction in predicting `dprime` (p = .07). 

Together, this is suggestive that the **strength** or even **presence** of an effect depends on an individual's language ability. But *why* might this be?

One intuitive (but speculative) explanation is that non-native speakers engage in less sensorimotor simulation when comprehending language; thus, the effect of `match` is weakened or nonexistent because there is no stored visual representation with which to compare the target picture in the recall stage. Note that this interpretation need not posit that non-native speakers engage in *no* simulation: the implication would be that they do not simulate details about implied *orientation* specifically.

It's also worth noting that **accuracy** for the plausibility judgments (both fillers and critical trials) appears lower for the non-native speakers relative to the native speakers.

```{r}
df_e1 %>%
  filter(task == "plausibility") %>%
  group_by(native_speaker) %>%
  summarise(accuracy = mean(correct))

df_e1 %>%
  filter(task == "plausibility") %>%
  mutate(critical = correct_response) %>%
  mutate(correct_numeric = as.numeric(correct)) %>%
  ggplot(aes(x = native_speaker,
             y = correct_numeric,
             color = critical)) +
  stat_summary (fun = function(x){mean(x)},
                fun.min = function(x){mean(x) - 2*sd(x)/sqrt(length(x))},
                fun.max = function(x){mean(x) + 2*sd(x)/sqrt(length(x))},
                geom= 'pointrange', 
                position=position_dodge(width=0.95)) +
  scale_y_continuous(limits = c(0, 1)) +
  geom_hline(yintercept = .5, linetype = "dotted") +
  labs(x = "Native speaker",
       y = "Accuracy") +
  theme_minimal()
```


