---
title: "Simulation of implied shape"
author: "Sean Trott"
date: "10/14/2020"
output:
  html_document:
    toc: yes
    toc_float: yes
    # code_folding: hide
  pdf_document: default
  word_document:
    toc: yes
---

```{r include=FALSE}
library(tidyverse)
library(lme4)
```



# Load data

```{r}
# setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")

## First, load data about individual items
df_items = read_csv("../../data/items.csv")
nrow(df_items)

df_items_gathered = df_items %>%
  pivot_longer(c(list_1, list_2, list_3, list_4), 
               names_to = "list", values_to = "version")
nrow(df_items_gathered)

## Now, load actual experimental data

df_e1 = read_csv("../../data/processed/translation_e1_processed.csv")
length(unique(df_e1$subject))

# Double-check experimental data
# There should be 120 rows for both tasks combined for each subject, so divide amount per subject by 120
table(df_e1$task) / 120

```



## Preprocess and merge data

Set response keys for experimental data.

```{r}
df_e1$response = fct_recode(
  factor(df_e1$key_press),
  "Yes" = "89",
  "No" = "78"
)

```

Indicate the correct response, based on whether item was a filler ("No") or not ("Yes").

```{r}
df_e1 = df_e1 %>%
  mutate(correct_response = case_when(
    type == "filler" ~ "No",
    is.na(type) ~ "Yes"
  )) %>%
  mutate(correct = correct_response == response)
```


**TODO**: Identify and exclude ppts with low filler accuracy?

```{r}
df_ppt_accuracy_plaus = df_e1 %>%
  filter(type == "filler") %>%
  filter(task == "plausibility") %>%
  group_by(subject) %>%
  summarise(filler_accuracy = mean(correct))
```


Now merge with items, and label for whether the picture shape matches the sentence shape.

```{r}
df_merged = df_e1 %>%
  left_join(df_items_gathered, on = c(list, object))

## Label whether or not shape/sentence is listed as matching
df_merged = df_merged %>%
  mutate(
    match = case_when(
      type == "filler" ~ "filler",
      version %in% c('aa', 'bb') ~ "yes",
      (version %in% c('aa', 'bb')) == FALSE ~ "no"
    )
  )

df_merged = df_e1 %>%
  left_join(df_ppt_accuracy_plaus, on = c(subject))


```


# Analyses

## Does matching orientation predict accuracy on recall items?

```{r}
df_merged_recall_critical = df_merged %>%
  filter(task == "recall") %>%
  filter(match != "filler")


df_merged_recall_critical %>%
  group_by(match) %>%
  summarise(accuracy = mean(correct))

### Unbalanced number of matching vs. mismatching: not an issue with "items", seems to be an issue in how they're distributed on list 1 and list 3
df_merged_recall_critical %>%
  group_by(match, subject, list) %>%
  summarise(accuracy = mean(correct),
            count = n())


df_merged_recall_critical %>%
  mutate(correct_numeric = as.numeric(correct)) %>%
  ggplot(aes(x = match,
             y = correct_numeric)) +
  stat_summary (fun = function(x){mean(x)},
                fun.min = function(x){mean(x) - 2*sd(x)/sqrt(length(x))},
                fun.max = function(x){mean(x) + 2*sd(x)/sqrt(length(x))},
                geom= 'pointrange', 
                position=position_dodge(width=0.95)) +
  scale_y_continuous(limits = c(0, 1)) +
  geom_hline(yintercept = .5, linetype = "dotted") +
  labs(x = "Matching shape",
       y = "Accuracy") +
  theme_minimal()

model_full = glmer(data = df_merged_recall_critical,
                   correct ~ match + 
                     (1 + match | subject) +
                     (1 + match | object),
                   family = binomial())

model_reduced = glmer(data = df_merged_recall_critical,
                   correct ~ 
                     (1 + match | subject) +
                     (1 + match | object),
                   family = binomial())

summary(model_full)
anova(model_full, model_reduced)

```


## Does participant-level d-prime vary for match vs. mismatch orientation?

**TODO**...
