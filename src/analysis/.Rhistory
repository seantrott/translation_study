df_just_math %>%
drop_na(Bet) %>%
group_by(Question, Target) %>%
summarise(mean_bet = mean(Bet),
sd_bet = sd(Bet))
model_full = lmer(data = df_just_math,
Bet ~ Target +
(1 | Order) +
(1 | Participant),
# (1 | Question)
REML = FALSE)
model_reduced = lmer(data = df_just_math,
Bet ~ # Target +
(1 | Order) +
(1 | Participant),
# (1 | Question),
REML = FALSE)
anova(model_full, model_reduced)
library(tidyverse)
library(lme4)
calculate_MI = function(df) {
# First, calculate p(m) and p(w)
# p(m) = #meaning occurrences / size of dataset
# p(w) = #wordform occurrences / size of dataset
df = df %>%
mutate(pm = freq_adjusted / sum(df$freq_adjusted)) %>%
group_by(wordform) %>%
mutate(wf = sum(freq_adjusted)) %>%
ungroup() %>%
mutate(pw = wf / sum(df$freq_adjusted))
## calculate p(m|w)
## p(m|w) = #meaning occurrences / #wordform occurrences
## p(m & w) = p(m|w) * p(w)
df = df %>%
## calculate p(m|w)
mutate(cond_prob = freq_adjusted / wf) %>%
## calculate p(m&w)
mutate(joint_prob = cond_prob * pw) %>%
mutate(mi = joint_prob * log(joint_prob/(pw*pm)))
# Return df
df
}
```
This is a helper function to calculate KLD for each word/meaning pairing.
```{r}
calculate_kld = function(df) {
# E[KLD] = p(m) * log(1/p(m|w))
df = df %>%
mutate(kld = pm * log(1/cond_prob))
df
}
```
This is a helper function for calculating entropy and other information about overall ambiguity.
```{r}
calculate_ambiguity = function(df) {
df_entropy = df %>%
## TODO: Get normalized surprisal
group_by(wordform) %>%
summarise(entropy = -sum(cond_prob * log(cond_prob)),
total_frequency = mean(total_frequency),
meanings = n(),
homophones = n() - 1,
ambiguous = homophones > 0)
df_entropy
}
```
## Baseline functions
These functions create the various baselines.
```{r}
## Creates baseline weighted by phonotactic plausibilty of wordforms.
## TODO: Store metrics instead of entire datafram
create_baselines_phonotactic_weighting = function(df_wordforms, df_lemmas, N) {
# Initialize random seed each time function is called
set.seed(1)
# Create dataframe to store each baseline
df_all = data.frame()
# For each iteration...
for (n in c(1:N)) {
df_new = data.frame()
for (syll in unique(df_lemmas$num_sylls_est)) {
# Get lemmas of appropriate length
df_tmp_lemmas = df_lemmas %>%
filter(num_sylls_est == syll)
# Get wordforms of appropriate length
df_tmp_wordforms = df_wordforms %>%
filter(num_sylls_est == syll)
# sample from wordforms
df_sampled_w = df_tmp_wordforms %>%
sample_n(size = nrow(df_tmp_lemmas),
replace = TRUE,
weight = p_normalized)
# Add wordform and normalized probability, just in case we need it
df_tmp_lemmas$wordform = df_sampled_w$wordform
df_tmp_lemmas$p_normalized = df_sampled_w$p_normalized
# Add to df_new
df_new = df_new %>%
bind_rows(df_tmp_lemmas)
# Set lexicon number
df_new$lexicon = n
df_new = df_new %>%
calculate_MI() %>%
calculate_kld()
}
df_all = df_all %>%
bind_rows(df_new)
}
df_all
}
```
This function helps calculate the mean MI/KL-D for each artificial lexicon.
```{r}
calculate_metrics = function(df) {
df_mi = tibble()
for (l in unique(df$lexicon)) {
# Get target lexicon
df_lex = df %>%
filter(lexicon == l)
# Calculate MI scores
df_lex = df_lex %>%
calculate_MI() %>%
mutate(kld = pm * log(1/cond_prob))
# Calculate entropy
df_entropy = df_lex %>%
calculate_ambiguity()
# Calculate sum(MI)
mi = df_lex %>% summarise(MI = sum(mi),
lex = mean(lexicon),
KLD = sum(kld))
# Calculate mean entropy, etc.
mi = df_entropy %>%
summarise(Entropy = mean(entropy),
avg_meanings = mean(meanings),
max_meanings = max(meanings),
median_meanings = median(meanings),
homophony_rate = mean(ambiguous)) %>%
bind_cols(mi)
# Bind with other lexica info
df_mi = df_mi %>%
bind_rows(mi)
}
df_mi
}
df_japanese = read_csv("../data/processed/japanese/reals/japanese_with_mps_4phone_holdout.csv")
nrow(df_japanese)
# df_japanese = read_csv("../data/processed/japanese/reals/japanese_with_lstm.csv")
# nrow(df_japanese)
df_japanese = df_japanese %>%
# This is important to get the baseline functions to work (references "wordform" column)
mutate(wordform = phonetic_remapped) %>%
mutate(p = 10 ** heldout_log_prob) %>%
group_by(num_sylls_est) %>%
mutate(total_p = sum(p)) %>%
mutate(p_normalized = p / total_p)
## Add #sylls
df_sylls = df_japanese %>%
select(num_sylls_est, wordform)
```
Next, load the lemmas with their frequency estimates.
```{r}
df_japanese_lemmas = read_csv("../data/processed/japanese/reals/japanese_all_reals_4phone.csv")
nrow(df_japanese_lemmas)
df_japanese_lemmas = df_japanese_lemmas %>%
mutate(wordform = phonetic_remapped) %>%
mutate(freq_adjusted = frequency +1) %>%
group_by(phonetic_remapped) %>%
mutate(total_frequency = sum(freq_adjusted),
relative_frequency = freq_adjusted / total_frequency) %>%
ungroup()
nrow(df_japanese_lemmas)
## Add #sylls
df_japanese_lemmas = df_japanese_lemmas %>%
left_join(df_sylls, on = wordform) %>%
drop_na(num_sylls_est)
df_japanese_lemmas$lemma_id = c(1:nrow(df_japanese_lemmas))
setwd("/Users/seantrott/Dropbox/UCSD/Research/Ambiguity/Evolution/homophony_delta/analyses")
df_japanese = read_csv("../data/processed/japanese/reals/japanese_with_mps_4phone_holdout.csv")
nrow(df_japanese)
# df_japanese = read_csv("../data/processed/japanese/reals/japanese_with_lstm.csv")
# nrow(df_japanese)
df_japanese = df_japanese %>%
# This is important to get the baseline functions to work (references "wordform" column)
mutate(wordform = phonetic_remapped) %>%
mutate(p = 10 ** heldout_log_prob) %>%
group_by(num_sylls_est) %>%
mutate(total_p = sum(p)) %>%
mutate(p_normalized = p / total_p)
## Add #sylls
df_sylls = df_japanese %>%
select(num_sylls_est, wordform)
df_japanese_lemmas = read_csv("../data/processed/japanese/reals/japanese_all_reals_4phone.csv")
nrow(df_japanese_lemmas)
df_japanese_lemmas = df_japanese_lemmas %>%
mutate(wordform = phonetic_remapped) %>%
mutate(freq_adjusted = frequency +1) %>%
group_by(phonetic_remapped) %>%
mutate(total_frequency = sum(freq_adjusted),
relative_frequency = freq_adjusted / total_frequency) %>%
ungroup()
nrow(df_japanese_lemmas)
## Add #sylls
df_japanese_lemmas = df_japanese_lemmas %>%
left_join(df_sylls, on = wordform) %>%
drop_na(num_sylls_est)
df_japanese_lemmas$lemma_id = c(1:nrow(df_japanese_lemmas))
df_japanese_lemmas = df_japanese_lemmas %>%
calculate_MI() %>%
mutate(kld = pm * log(1/cond_prob))
## Calculate summary MI and KL-D
real_japanese = df_japanese_lemmas %>%
summarise(KLD = sum(kld),
MI = sum(mi))
real_japanese
## Also calculate entropy
df_japanese_entropy = df_japanese_lemmas %>%
calculate_ambiguity()
## Bind with real_english
real_japanese = df_japanese_entropy %>%
summarise(Entropy = mean(entropy),
avg_meanings = mean(meanings),
max_meanings = max(meanings),
median_meanings = median(meanings),
homophony_rate = mean(ambiguous)) %>%
bind_cols(real_japanese)
real_japanese
df_new_japanese = create_baselines_phonotactic_weighting(df_japanese,
df_japanese_lemmas,
N = 10)
df_new_japanese = create_baselines_phonotactic_weighting(df_japanese,
df_japanese_lemmas,
N = 10)
```
Now, we calculate the mutual information and KL-D for each baseline lexicon.
```{r}
df_mi_japanese = df_new_japanese %>%
calculate_metrics()
library(tidyverse)
library(lme4)
calculate_MI = function(df) {
# First, calculate p(m) and p(w)
# p(m) = #meaning occurrences / size of dataset
# p(w) = #wordform occurrences / size of dataset
df = df %>%
mutate(pm = freq_adjusted / sum(df$freq_adjusted)) %>%
group_by(wordform) %>%
mutate(wf = sum(freq_adjusted)) %>%
ungroup() %>%
mutate(pw = wf / sum(df$freq_adjusted))
## calculate p(m|w)
## p(m|w) = #meaning occurrences / #wordform occurrences
## p(m & w) = p(m|w) * p(w)
df = df %>%
## calculate p(m|w)
mutate(cond_prob = freq_adjusted / wf) %>%
## calculate p(m&w)
mutate(joint_prob = cond_prob * pw) %>%
mutate(mi = joint_prob * log(joint_prob/(pw*pm)))
# Return df
df
}
calculate_kld = function(df) {
# E[KLD] = p(m) * log(1/p(m|w))
df = df %>%
mutate(kld = pm * log(1/cond_prob))
df
}
calculate_ambiguity = function(df) {
df_entropy = df %>%
## TODO: Get normalized surprisal
group_by(wordform) %>%
summarise(entropy = -sum(cond_prob * log(cond_prob)),
total_frequency = mean(total_frequency),
meanings = n(),
homophones = n() - 1,
ambiguous = homophones > 0)
df_entropy
}
create_baselines_phonotactic_weighting = function(df_wordforms, df_lemmas, N) {
# Initialize random seed each time function is called
set.seed(1)
# Create dataframe to store each baseline
df_all = data.frame()
# For each iteration...
for (n in c(1:N)) {
df_new = data.frame()
for (syll in unique(df_lemmas$num_sylls_est)) {
# Get lemmas of appropriate length
df_tmp_lemmas = df_lemmas %>%
filter(num_sylls_est == syll)
# Get wordforms of appropriate length
df_tmp_wordforms = df_wordforms %>%
filter(num_sylls_est == syll)
# sample from wordforms
df_sampled_w = df_tmp_wordforms %>%
sample_n(size = nrow(df_tmp_lemmas),
replace = TRUE,
weight = p_normalized)
# Add wordform and normalized probability, just in case we need it
df_tmp_lemmas$wordform = df_sampled_w$wordform
df_tmp_lemmas$p_normalized = df_sampled_w$p_normalized
# Add to df_new
df_new = df_new %>%
bind_rows(df_tmp_lemmas)
# Set lexicon number
df_new$lexicon = n
df_new = df_new %>%
calculate_MI() %>%
calculate_kld()
}
df_all = df_all %>%
bind_rows(df_new)
}
df_all
}
calculate_metrics = function(df) {
df_mi = tibble()
for (l in unique(df$lexicon)) {
# Get target lexicon
df_lex = df %>%
filter(lexicon == l)
# Calculate MI scores
df_lex = df_lex %>%
calculate_MI() %>%
mutate(kld = pm * log(1/cond_prob))
# Calculate entropy
# df_entropy = df_lex %>%
#   calculate_ambiguity()
# Calculate sum(MI)
mi = df_lex %>% summarise(MI = sum(mi),
lex = mean(lexicon),
KLD = sum(kld))
# Calculate mean entropy, etc.
# mi = df_entropy %>%
#   summarise(Entropy = mean(entropy),
#             avg_meanings = mean(meanings),
#             max_meanings = max(meanings),
#            median_meanings = median(meanings),
#            homophony_rate = mean(ambiguous)) %>%
#  bind_cols(mi)
# Bind with other lexica info
df_mi = df_mi %>%
bind_rows(mi)
}
df_mi
}
setwd("/Users/seantrott/Dropbox/UCSD/Research/Ambiguity/Evolution/homophony_delta/analyses")
df_english = read_csv("../data/processed/english/reals/english_with_mps_5phone_holdout.csv")
df_english = df_english %>%
# This is important to get the baseline functions to work (references "wordform" column)
mutate(wordform = PhonDISC) %>%
mutate(p = 10 ** heldout_log_prob) %>%
group_by(num_sylls_est) %>%
mutate(total_p = sum(p)) %>%
mutate(p_normalized = p / total_p)
## Add #sylls
df_sylls = df_english %>%
select(num_sylls_est, PhonDISC)
df_english_lemmas = read_delim("../data/frequency/english/lemmas.csv", delim = "\\",
quote = "")
nrow(df_english_lemmas)
df_english_lemmas = df_english_lemmas %>%
mutate(freq_adjusted = Cob + 1) %>%
group_by(PhonDISC) %>%
mutate(total_frequency = sum(freq_adjusted),
relative_frequency = freq_adjusted / total_frequency) %>%
ungroup()
## Add #sylls
df_english_lemmas = df_english_lemmas %>%
left_join(df_sylls, on = PhonDISC) %>%
drop_na(num_sylls_est)
df_english_lemmas$lemma_id = c(1:nrow(df_english_lemmas))
df_english_lemmas = df_english_lemmas %>%
mutate(wordform = PhonDISC) %>%
calculate_MI() %>%
## not sure this is right
mutate(kld = pm * log(1/cond_prob))
## Calculate summary MI and KL-D
real_english = df_english_lemmas %>%
summarise(KLD = sum(kld),
MI = sum(mi))
real_english
df_new_english = create_baselines_phonotactic_weighting(df_english,
df_english_lemmas,
N = 10)
df_mi_english = df_new_english %>%
calculate_metrics()
df_mi_english %>%
ggplot(aes(x = MI,
y = KLD)) +
geom_point(alpha = .5,
size = 2) +
geom_point(data = real_english,
aes(x = MI,
y = KLD),
color = "Blue",
alpha = .3,
size = 5) +
theme_bw()
df_one_english = df_new_english %>%
filter(lexicon == 1) %>%
calculate_MI() %>%
mutate(kld = pm * log(1/cond_prob))
ggplot() +
geom_density(alpha = .5,
data = df_one_english,
aes(x = cond_prob, fill = "Baseline")) +
geom_density(alpha = .5,
data = df_english_lemmas,
aes(x = cond_prob, fill = "Real")) +
scale_fill_manual(name = "",
values = c("Baseline" = "Red",
"Real" = "Light Blue")) +
labs(x = "P(m|w)") +
theme_bw()
df_mandarin = read_csv("../data/processed/mandarin_cld/reals/mandarin_cld_with_mps_4phone_holdout.csv")
nrow(df_mandarin)
df_mandarin = df_mandarin %>%
# This is important to get the baseline functions to work (references "wordform" column)
mutate(wordform = phonetic_remapped) %>%
mutate(p = 10 ** heldout_log_prob) %>%
group_by(num_sylls_est) %>%
mutate(total_p = sum(p)) %>%
mutate(p_normalized = p / total_p)
## Add #sylls
df_sylls = df_mandarin %>%
select(num_sylls_est, wordform)
```
Next, load the lemmas with their frequency estimates.
```{r}
df_mandarin_lemmas = read_csv("../data/processed/mandarin_cld/reals/mandarin_cld_all_reals_4phone.csv")
nrow(df_mandarin_lemmas)
df_mandarin_lemmas = df_mandarin_lemmas %>%
mutate(wordform = phonetic_remapped) %>%
mutate(freq_adjusted = FrequencyRaw +1) %>%
group_by(phonetic_remapped) %>%
mutate(total_frequency = sum(freq_adjusted),
relative_frequency = freq_adjusted / total_frequency) %>%
ungroup()
nrow(df_mandarin_lemmas)
## Add #sylls
df_mandarin_lemmas = df_mandarin_lemmas %>%
left_join(df_sylls, on = wordform) %>%
drop_na(num_sylls_est)
df_mandarin_lemmas$lemma_id = c(1:nrow(df_mandarin_lemmas))
```
## Calculate scores for real lexicon
```{r}
df_mandarin_lemmas = df_mandarin_lemmas %>%
calculate_MI() %>%
mutate(kld = pm * log(1/cond_prob))
## Calculate summary MI and KL-D
real_mandarin = df_mandarin_lemmas %>%
summarise(KLD = sum(kld),
MI = sum(mi))
real_mandarin
## Also calculate entropy
df_mandarin_entropy = df_mandarin_lemmas %>%
calculate_ambiguity()
## Bind with real_english
real_mandarin = df_mandarin_entropy %>%
summarise(Entropy = mean(entropy),
avg_meanings = mean(meanings),
max_meanings = max(meanings),
median_meanings = median(meanings),
homophony_rate = mean(ambiguous)) %>%
bind_cols(real_mandarin)
real_mandarin
df_new_mandarin = create_baselines_phonotactic_weighting(df_mandarin,
df_mandarin_lemmas,
N = 10)
g
df_mi_mandarin = df_new_mandarin %>%
calculate_metrics()
df_mi_mandarin %>%
ggplot(aes(x = MI,
y = KLD)) +
geom_point(alpha = .5,
size = 2) +
geom_point(data = real_mandarin,
aes(x = MI,
y = KLD),
color = "Blue",
alpha = .3,
size = 5) +
theme_bw()
library(tidyverse)
library(lme4)
setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")
## First, load data about individual items
df_items = read_csv("../../data/items.csv")
nrow(df_items)
df_items_gathered = df_items %>%
pivot_longer(c(list_1, list_2, list_3, list_4),
names_to = "list", values_to = "version")
nrow(df_items_gathered)
df_e1 = read_csv("../../data/processed/translation_mandarin_pilot_processed.csv")
length(unique(df_e1$subject))
table(df_e1$task) / 120
df_e1$task
setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")
## First, load data about individual items
df_items = read_csv("../../data/items.csv")
nrow(df_items)
df_items_gathered = df_items %>%
pivot_longer(c(list_1, list_2, list_3, list_4),
names_to = "list", values_to = "version")
nrow(df_items_gathered)
## Now, load actual experimental data
df_e1 = read_csv("../../data/processed/translation_e1_processed.csv")
length(unique(df_e1$subject))
# Double-check experimental data
# There should be 120 rows for both tasks combined for each subject, so divide amount per subject by 120
table(df_e1$task) / 120
