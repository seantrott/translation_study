pivot_longer(c(a, b), names_to = 'treatment', values_to = 'result')
df_messy_2
df_messy_2
df_messy_2$age = c(24, 25, 26)
df_messy_2
df_messy_2 %>%
pivot_longer(c(a, b), names_to = 'treatment', values_to = 'result')
mtcars
mtcars %>%
filter(mpg > mean(mtcars$mpg))
mean(mtcars$mpg)
df_tidy %>%
filter(name == "john")
df_tidy %>%
filter(treatment == "john")
df_tidy %>%
filter(treatment == "a")
df_tidy %>%
filter(treatment == "a") %>%
ggplot(aes(x = result)) +
geom_histogram()
df_tidy %>%
filter(treatment == "b") %>%
ggplot(aes(x = result)) +
geom_histogram()
df_tidy %>%
sample_n(2, replace=FALSE)
df_tidy
df_tidy %>%
summarise(avg = mean(result),
sd = sd(result),
count = n())
df_tidy %>%
group_by(treatment) %>%
summarise(mean_outcome = mean(result),
sd_outcome = sd(result),
count = n())
df_tidy %>%
group_by(name) %>%
summarise(mean_outcome = mean(result),
sd_outcome = sd(result),
count = n())
df_tidy
df_tidy %>%
group_by(name, treatment) %>%
summarise(mean_outcome = mean(result),
sd_outcome = sd(result),
count = n())
iris
iris %>%
mutate(sepal = Sepal.Width + Sepal.Length) %>%
head(5)
iris %>%
mutate(sepal = Sepal.Width + Sepal.Length,
petal = Petal.Width + Petal.Length) %>%
head(5)
iris %>%
mutate(sepal = Sepal.Width + Sepal.Length,
petal = Petal.Width + Petal.Length) %>%
mutate(total = sepal + petal) %>%
head(5)
iris$Species
iris %>%
group_by(Species) %>%
summarise(mean_sepal_width = mean(Sepal.Width),
mean_sepal_length = mean(Sepal.Length),
mean_petal_width = mean(Petal.Width),
mean_petal_length = mean(Petal.Length))
iris %>%
group_by(Species) %>%
summarise(mean_sepal_width = mean(Sepal.Width),
mean_sepal_length = mean(Sepal.Length),
mean_petal_width = mean(Petal.Width),
mean_petal_length = mean(Petal.Length)) %>%
mutate(sepal = mean_sepal_width + mean_sepal_length,
petal = mean_petal_width + mean_petal_length) %>%
mutate(total = sepal + petal)
df_inds = data.frame(
name = c("john", "mary", "jane"),
age = c(24, 45, 32),
gender = c("M", "F", "F")
)
df_inds
df_tidy
df_inds
df_merged = df_tidy %>%
left_join(df_inds, by="name")
df_merged
df_inds
mtcars %>%
ggplot() +
geom_point(aes(x = wt,
y = mpg))
basic_plot = mtcars %>%
ggplot() +
geom_point(aes(x = wt,
y = mpg,
color = am))
basic_plot
basic_plot = mtcars %>%
ggplot() +
geom_point(aes(x = wt,
y = mpg,
color = factor(am)))
basic_plot
?mtcars
basic_plot = basic_plot + theme_minimal()
basic_plot
mtcars$transmission = fct_recode(
factor(mtcars$am),
"auto" = "0",
"manual" = "1"
)
basic_plot = basic_plot +
aes(color=factor(am))
basic_plot
basic_plot = basic_plot +
aes(size = hp)
basic_plot
basic_plot = basic_plot +
aes(shape = factor(vs))
basic_plot
mtcars %>%
ggplot() +
geom_point(aes(x = wt,
y = mpg,
color = factor(am),
size = hp,
shape = factor(vs)),
alpha = .8) +
theme_minimal()
library(ordinal)
exp(3.06)
library(corrplot)
df_norms = df_merged %>%
group_by(word, same, ambiguity_type, version_with_order) %>%
summarise(mean_relatedness = mean(relatedness),
median_relatedness = median(relatedness),
count = n(),
sd_relatedness = sd(relatedness),
distance_bert = mean(distance_bert),
distance_elmo = mean(distance_elmo))
library(tidyverse)
df_norms = df_merged %>%
group_by(word, same, ambiguity_type, version_with_order) %>%
summarise(mean_relatedness = mean(relatedness),
median_relatedness = median(relatedness),
count = n(),
sd_relatedness = sd(relatedness),
distance_bert = mean(distance_bert),
distance_elmo = mean(distance_elmo))
# Statistical rethinking, chapter 5
library(rethinking)
## Divorce rates
data(WaffleDivorce)
d <- WaffleDivorce
# standardize variables
d$A <- scale( d$MedianAgeMarriage )
d$D <- scale( d$Divorce )
data(WaffleDivorce)
d <- WaffleDivorce
# standardize variables
d$A <- scale( d$MedianAgeMarriage )
d$D <- scale( d$Divorce )
m5.1 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + bA * A ,
a ~ dnorm( 0 , 0.2 ) ,
bA ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
summary(m5.1)
set.seed(10)
prior <- extract.prior( m5.1 )
mu <- link( m5.1 , post=prior , data=list( A=c(-2,2) ) )
plot( NULL , xlim=c(-2,2) , ylim=c(-2,2) )
for ( i in 1:50 )
lines( c(-2,2) , mu[i,] , col=col.alpha("black",0.4) )
A_seq <- seq( from=-3 , to=3.2 , length.out=30 )
mu <- link( m5.1 , data=list(A=A_seq) )
mu.mean <- apply( mu , 2, mean )
mu.PI <- apply( mu , 2 , PI )
plot( D ~ A , data=d , col=rangi2 )
lines( A_seq , mu.mean , lwd=2 )
shade( mu.PI , A_seq )
precise(mu)
precis(mu)
mu]
mu
precis(mu)
precis(m5.1)
## Draing DAGs: "daggity"
library(dagitty)
dag5.1 <- dagitty( "dag { A -> D A -> M M -> D
}")
coordinates(dag5.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )
drawdag( dag5.1 )
DMA_dag2 <- dagitty('dag{ D <- A -> M }')
impliedConditionalIndependencies( DMA_dag2 )
impliedConditionalIndependencies(dag5.1)
DMA_dag2 <- dagitty('dag{ D <- A <- M }')
impliedConditionalIndependencies( DMA_dag2 )
drawdag(drawdag)
coordinates(DMA_dag2) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )
drawdag(drawdag)
drawdag( DMA_dag2 )
impliedConditionalIndependencies( DMA_dag2 )
DMA_dag2 <- dagitty('dag{ D <- M -> M }')
drawdag( DMA_dag2 )
impliedConditionalIndependencies( DMA_dag2 )
DMA_dag2 <- dagitty('dag{ D <- M -> A }')
drawdag( DMA_dag2 )
impliedConditionalIndependencies( DMA_dag2 )
DMA_dag2 <- dagitty('dag{ D <- A -> M }')
impliedConditionalIndependencies( DMA_dag2 )
dag_a1 <- dagitty( "dag { context -> RT }")
drawdag( dag_a1 )
dag_a1 <- dagitty( "dag { context distance -> RT }")
drawdag( dag_a1 )
dag_a1 <- dagitty( "dag { same context_distance -> RT }")
drawdag( dag_a1 )
dag_a1 <- dagitty( "dag { AT boundary context -> RT }")
drawdag( dag_a1 )
impliedConditionalIndependencies( dag_a1 )
dag_a1 <- dagitty( "dag { AT SB CNXT -> RT }")
drawdag( dag_a1 )
impliedConditionalIndependencies( dag_a1 )
dag_a2 <- dagitty( "dag { AT CNXT -> RT <- SB }")
drawdag( dag_a2 )
impliedConditionalIndependencies( dag_a2 )
dag_a1b <- dagitty( "dag { AT SB -> CNXT -> RT }")
drawdag( dag_a1b )
impliedConditionalIndependencies( dag_a1b )
m5.3 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + bM*M + bA*A ,
a ~ dnorm( 0 , 0.2 ) ,
bM ~ dnorm( 0 , 0.5 ) ,
bA ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
precis( m5.3 )
m5.3 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + bM*M + bA*A ,
a ~ dnorm( 0 , 0.2 ) ,
bM ~ dnorm( 0 , 0.5 ) ,
bA ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
m5.3 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + bM*M + bA*A ,
a ~ dnorm( 0 , 0.2 ) ,
bM ~ dnorm( 0 , 0.5 ) ,
bA ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
d$A <- scale( d$MedianAgeMarriage )
d$M <- scale( d$Marriage )
d$D <- scale( d$Divorce )
m5.3 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + bM*M + bA*A ,
a ~ dnorm( 0 , 0.2 ) ,
bM ~ dnorm( 0 , 0.5 ) ,
bA ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
precis( m5.3 )
plot( coeftab(m5.1,m5.2,m5.3), par=c("bA","bM") )
plot( coeftab(m5.3), par=c("bA","bM") )
coeftab(m5.3)
m5.2 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + bM * M ,
a ~ dnorm( 0 , 0.2 ) ,
bM ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
summary(m5.2)
plot( coeftab(m5.1, m5.2, m5.3), par=c("bA","bM") )
N <- 50 # number of simulated States age <- rnorm( N )
# sim A
mar <- rnorm( N , -age ) #
div <- rnorm( N , age )
age <- rnorm( N )
mar <- rnorm( N , -age )
div <- rnorm( N , age )
plot(age, mar)
plot(age, div)
plot(mar, div)
?rnorm
mar <- rnorm( N , -age, sd = 5 )
plot(age, mar)
m5.4 <- quap(
alist(
M ~ dnorm( mu , sigma ) ,
mu <- a + bAM * A ,
a ~ dnorm( 0 , 0.2 ) ,
bAM ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
mu <- link(m5.4)
mu_mean <- apply( mu , 2 , mean )
mu_resid <- d$M - mu_mean
1- pnorm(68, 70, 3)
pnorm(68, 70, 3, lower.tail=FALSE)
?prnom
?pnorm
67 + .5 * 2.5
67 - .5 * 2.5
pnorm(68.25, 64.5, 2.5) - (1-pnorm(65.275, 64.5, 2.5))
qnorm(.96.2, 70, 3)
qnorm(.962, 70, 3)
qnorm(.962, 0, 1)
qnorm(.984, 64.5,2.5)
pnorm(64, 65,3)
qnorm(.3694, 0,1)
pnorm(64, 70,3)
qnorm(.0227, 0, 1)
qnorm(.0228, 0, 1)
pnorm(64, 70, 3)
.96 - .0228
pnorm(76.29, 64.5, 2.5)
qnorm(.0228, 64.5, 3)
1 - pnorm(63.7, 64.5, 2.5)
?qnorm
pnorm(.9, mean = 50, sd = 5)
qnorm(p = .9, mean = 50, sd = 5)
qnorm(p = .1, mean = 50, sd = 5)
pnorm(q = .9, mean = 531, sd = 104)
qnorm(p = .9, mean = 531, sd = 104)
log(.3) - log(.1)
log(4) - log(2)
log(.6) - log(.2)
log(.6) - log(.15)
# Statistical rethinking, chapter 2
library(tidyverse)
library(rethinking)
dbinom( 6 , size=9 , prob=0.5 )
DATA = 6 ## 6 observed water
NUM_EVENTS = 9 ## 9 events
df_probs = data.frame(hypothesis = seq(0, 1, .01)) ## Various hypotheses for p(W)
df_probs = df_probs %>%
mutate(prior = 1/length(hypothesis),
likelihood = dbinom(DATA , size=NUM_EVENTS, prob=hypothesis)) %>%
mutate(posterior = prior * likelihood)
hist(df_probs$prior)
df_probs %>%
ggplot(aes(x = hypothesis,
y = likelihood)) +
geom_point() +
theme_minimal()
df_probs %>%
ggplot(aes(x = hypothesis,
y = posterior)) +
geom_point() +
theme_minimal()
# Statistical rethinking, chapter 6
library(rethinking)
### Multicollineraity: legs and height
N <- 100 # number of individuals
set.seed(909)
height <- rnorm(N,10,2)
# sim total height of each
leg_prop <- runif(N,0.4,0.5)
# leg as proportion of height
leg_left <- leg_prop*height +
# sim left leg as proportion + error
rnorm( N , 0 , 0.02 )
leg_right <- leg_prop*height + # sim right leg as proportion + error
rnorm( N , 0 , 0.02 )
# combine into data frame
d <- data.frame(height,leg_left,leg_right)
m6.1 <- quap( alist(
height ~ dnorm( mu , sigma ) ,
mu <- a + bl*leg_left + br*leg_right ,
a ~ dnorm( 10 , 100 ) ,
bl ~ dnorm( 2 , 10 ) ,
br ~ dnorm( 2 , 10 ) ,
sigma ~ dexp( 1 )
) , data=d )
precis(m6.1)
plot(precis(m6.1))
post <- extract.samples(m6.1)
plot( bl ~ br , post , col=col.alpha(rangi2,0.1) , pch=16 )
sum_blbr <- post$bl + post$br
dens( sum_blbr , col=rangi2 , lwd=2 , xlab="sum of bl and br" )
N = 50
TARGET = c(5, 5)
df_norm = data.frame(
x = rnorm(mean = 5, sd = 1, n = N),
y = rnorm(mean = 5, sd = 1, n = N)
)
df_norm = data.frame(
x = rnorm(mean = 5, sd = 1, n = N),
y = rnorm(mean = 5, sd = 1, n = N)
)
df_norm
stats::dist
?stats::dist
dist
X1 = c(2, 2)
dist(X1, TARGET)
dist(X1)
dist(c(X1, TARGET))
tx = 5
ty = 5
X1 = c(2, 2)
df_norm = data.frame(
x = rnorm(mean = 5, sd = 1, n = N),
y = rnorm(mean = 5, sd = 1, n = N)
)
df_norm = df_norm %>%
mutate(
ed = sqrt((x - tx)^2 + (y-ty)^2)
)
library(tidyverse)
df_norm = data.frame(
x = rnorm(mean = 5, sd = 1, n = N),
y = rnorm(mean = 5, sd = 1, n = N)
)
df_norm = df_norm %>%
mutate(
ed = sqrt((x - tx)^2 + (y-ty)^2)
)
df_norm %>%
ggplot(aes(x = x,
y = y,
fill = ed)) +
geom_point()
df_norm %>%
ggplot(aes(x = x,
y = y,
color = ed)) +
geom_point()
tx1 = 5
tx2 = 5
df_norm = data.frame(
x1 = rnorm(mean = 5, sd = 1, n = N),
x2 = rnorm(mean = 5, sd = 1, n = N)
)
df_norm = df_norm %>%
mutate(
ed = sqrt((x1 - tx1)^2 + (x2-tx2)^2)
)
df_norm = data.frame(
x1 = rnorm(mean = 5, sd = 1, n = N),
x2 = rnorm(mean = 5, sd = 1, n = N),
y = rnorm(mean = 10, sd = 1, n = N)
)
df_norm = df_norm %>%
mutate(
ed = sqrt((x1 - tx1)^2 + (x2-tx2)^2)
)
m1 = lm(data = df_norm,
y ~ x1 * x2)
summary(m1)
m2 = lm(data = df_norm,
y ~ x1 + x2 + ed)
summary(m2)
1 - pnorm(3)
1 - pnorm(20)
pnorm(54, mean = 50, sd =8) - pnorm(46, mean = 50, sd = 8)
8 / sqrt(4)
pnorm(54, mean = 50, sd =4) - pnorm(46, mean = 50, sd = 4)
8 / sqrt(16)
pnorm(54, mean = 50, sd =2) - pnorm(46, mean = 50, sd = 2)
setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")
## First, load data about individual items
df_items = read_csv("../../data/items.csv")
nrow(df_items)
df_items_gathered = df_items %>%
pivot_longer(c(list_1, list_2, list_3, list_4),
names_to = "list", values_to = "version")
nrow(df_items_gathered)
## Now, load actual experimental data
df_e1 = read_csv("../../data/processed/translation_e1_processed.csv")
length(unique(df_e1$subject))
# Double-check experimental data
# There should be 120 rows for both tasks combined for each subject, so divide amount per subject by 120
table(df_e1$task) / 120
setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")
## First, load data about individual items
df_items = read_csv("../../data/items.csv")
nrow(df_items)
df_items_gathered = df_items %>%
pivot_longer(c(list_1, list_2, list_3, list_4),
names_to = "list", values_to = "version")
nrow(df_items_gathered)
library(tidyverse)
library(lme4)
setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")
df_e1 = read_csv("../../data/processed/translation_e1_processed.csv")
length(unique(df_e1$subject))
# Double-check experimental data
# There should be 120 rows for both tasks combined for each subject, so divide amount per subject by 120
table(df_e1$task) / 120
View(df_e1)
s = filter(df_e1, subject == "5n1k939z55")
View(s)
s = filter(df_e1, trial_index %in% c(483, 483, 485))
View(s)
nrow(s)
nrow(s) / 3
