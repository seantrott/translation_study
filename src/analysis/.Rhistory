sigma ~ dexp( 1 )
) , data = d )
summary(m5.1)
set.seed(10)
prior <- extract.prior( m5.1 )
mu <- link( m5.1 , post=prior , data=list( A=c(-2,2) ) )
plot( NULL , xlim=c(-2,2) , ylim=c(-2,2) )
for ( i in 1:50 )
lines( c(-2,2) , mu[i,] , col=col.alpha("black",0.4) )
A_seq <- seq( from=-3 , to=3.2 , length.out=30 )
mu <- link( m5.1 , data=list(A=A_seq) )
mu.mean <- apply( mu , 2, mean )
mu.PI <- apply( mu , 2 , PI )
plot( D ~ A , data=d , col=rangi2 )
lines( A_seq , mu.mean , lwd=2 )
shade( mu.PI , A_seq )
precise(mu)
precis(mu)
mu]
mu
precis(mu)
precis(m5.1)
## Draing DAGs: "daggity"
library(dagitty)
dag5.1 <- dagitty( "dag { A -> D A -> M M -> D
}")
coordinates(dag5.1) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )
drawdag( dag5.1 )
DMA_dag2 <- dagitty('dag{ D <- A -> M }')
impliedConditionalIndependencies( DMA_dag2 )
impliedConditionalIndependencies(dag5.1)
DMA_dag2 <- dagitty('dag{ D <- A <- M }')
impliedConditionalIndependencies( DMA_dag2 )
drawdag(drawdag)
coordinates(DMA_dag2) <- list( x=c(A=0,D=1,M=2) , y=c(A=0,D=1,M=0) )
drawdag(drawdag)
drawdag( DMA_dag2 )
impliedConditionalIndependencies( DMA_dag2 )
DMA_dag2 <- dagitty('dag{ D <- M -> M }')
drawdag( DMA_dag2 )
impliedConditionalIndependencies( DMA_dag2 )
DMA_dag2 <- dagitty('dag{ D <- M -> A }')
drawdag( DMA_dag2 )
impliedConditionalIndependencies( DMA_dag2 )
DMA_dag2 <- dagitty('dag{ D <- A -> M }')
impliedConditionalIndependencies( DMA_dag2 )
dag_a1 <- dagitty( "dag { context -> RT }")
drawdag( dag_a1 )
dag_a1 <- dagitty( "dag { context distance -> RT }")
drawdag( dag_a1 )
dag_a1 <- dagitty( "dag { same context_distance -> RT }")
drawdag( dag_a1 )
dag_a1 <- dagitty( "dag { AT boundary context -> RT }")
drawdag( dag_a1 )
impliedConditionalIndependencies( dag_a1 )
dag_a1 <- dagitty( "dag { AT SB CNXT -> RT }")
drawdag( dag_a1 )
impliedConditionalIndependencies( dag_a1 )
dag_a2 <- dagitty( "dag { AT CNXT -> RT <- SB }")
drawdag( dag_a2 )
impliedConditionalIndependencies( dag_a2 )
dag_a1b <- dagitty( "dag { AT SB -> CNXT -> RT }")
drawdag( dag_a1b )
impliedConditionalIndependencies( dag_a1b )
m5.3 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + bM*M + bA*A ,
a ~ dnorm( 0 , 0.2 ) ,
bM ~ dnorm( 0 , 0.5 ) ,
bA ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
precis( m5.3 )
m5.3 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + bM*M + bA*A ,
a ~ dnorm( 0 , 0.2 ) ,
bM ~ dnorm( 0 , 0.5 ) ,
bA ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
m5.3 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + bM*M + bA*A ,
a ~ dnorm( 0 , 0.2 ) ,
bM ~ dnorm( 0 , 0.5 ) ,
bA ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
d$A <- scale( d$MedianAgeMarriage )
d$M <- scale( d$Marriage )
d$D <- scale( d$Divorce )
m5.3 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + bM*M + bA*A ,
a ~ dnorm( 0 , 0.2 ) ,
bM ~ dnorm( 0 , 0.5 ) ,
bA ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
precis( m5.3 )
plot( coeftab(m5.1,m5.2,m5.3), par=c("bA","bM") )
plot( coeftab(m5.3), par=c("bA","bM") )
coeftab(m5.3)
m5.2 <- quap(
alist(
D ~ dnorm( mu , sigma ) ,
mu <- a + bM * M ,
a ~ dnorm( 0 , 0.2 ) ,
bM ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
summary(m5.2)
plot( coeftab(m5.1, m5.2, m5.3), par=c("bA","bM") )
N <- 50 # number of simulated States age <- rnorm( N )
# sim A
mar <- rnorm( N , -age ) #
div <- rnorm( N , age )
age <- rnorm( N )
mar <- rnorm( N , -age )
div <- rnorm( N , age )
plot(age, mar)
plot(age, div)
plot(mar, div)
?rnorm
mar <- rnorm( N , -age, sd = 5 )
plot(age, mar)
m5.4 <- quap(
alist(
M ~ dnorm( mu , sigma ) ,
mu <- a + bAM * A ,
a ~ dnorm( 0 , 0.2 ) ,
bAM ~ dnorm( 0 , 0.5 ) ,
sigma ~ dexp( 1 )
) , data = d )
mu <- link(m5.4)
mu_mean <- apply( mu , 2 , mean )
mu_resid <- d$M - mu_mean
library(tidyverse)
library(lme4)
library(ggridges)
library(ordinal)
setwd("/Users/seantrott/Dropbox/UCSD/Research/Ambiguity/SSD/trott_polysemy_experiment/src/analysis")
### Load preprocessed data
df_normed = read_csv("../../data/processed/polysemy_norming.csv")
### Filter to critical trials
df_normed_critical = df_normed %>%
filter(same %in% c(TRUE, FALSE)) %>%
filter(version_with_order != "catch")
length(unique(df_normed_critical$subject))
nrow(df_normed_critical)
table(df_normed_critical$same, df_normed_critical$ambiguity_type_oed)
### Recode version information to omit order, so it can be merged with distance information
df_normed_critical$version = fct_recode(
df_normed_critical$version_with_order,
M1_a_M1_b = "M1_b_M1_a",
M1_b_M2_a = "M2_a_M1_b",
M1_a_M2_a = "M2_a_M1_a",
M1_a_M2_b = "M2_b_M1_a",
M1_b_M2_b = "M2_b_M1_b",
M2_a_M2_b = "M2_b_M2_a"
)
# Preprocessing
## Bot checks
```{r}
df_ppt_bots = df_normed %>%
filter(type == "bot_check") %>%
mutate(b1_correct = B1 == 2,
b2_correct = B2 == "Chair")
df_bot_summ = df_ppt_bots %>%
group_by(subject) %>%
summarise(bot_avg = (b1_correct + b2_correct) / 2)
df_bot_summ
## Now remove ppts from critical stims that have < 100% avearge
df_normed_critical = df_normed_critical %>%
left_join(df_bot_summ, by = "subject") %>%
filter(bot_avg == 1)
length(unique(df_normed_critical$subject))
```
## Analyze catch trials
```{r}
### "Rose" should be "totally unrelated", and "blue" should be "same meaning"
df_catch = df_normed %>%
filter(same %in% c(TRUE, FALSE)) %>%
filter(version_with_order == "catch") %>%
mutate(correct_answer = case_when(
word == "rose" ~ 0,  ## Strict (totally unrelated)
word == "house" ~ 4  ## Strict (same meaning)
)) %>%
mutate(correct_response = relatedness == correct_answer)
###
df_ppts_catch = df_catch %>%
group_by(subject) %>%
summarise(catch_avg = mean(correct_response))
df_ppts_catch
## Now remove ppts from critical stims that have < 100% avearge
df_normed_critical = df_normed_critical %>%
left_join(df_ppts_catch, by = "subject") %>%
filter(catch_avg >= .5) # remove people who got less than 100% on the catch
length(unique(df_normed_critical$subject))
```
df_distances = read_csv("../../data/processed/stims_processed.csv")
nrow(df_distances)
df_merged = df_normed_critical %>%
left_join(df_distances, by = c("word", "version", "string", "overlap",
"source", "same", "Class", "ambiguity_type"))
nrow(df_merged)
length(unique(df_merged$subject))
df_merged$relatedness_ord = factor(df_merged$relatedness,
ordered = TRUE)
model_full <- clm(relatedness_ord ~ distance_elmo + distance_bert,
data = df_merged)
predict(model_full)
resid(model_full)
residuals(model_full)
predict.clm
predict(model_full, type = "response")
predict(model_full, type = "response")
?predict
predict(model_full, type = "prob")
df_merged$pred = predict(model_simple)
model_simple <- clm(relatedness_ord ~ distance_elmo + distance_bert,
data = df_merged)
df_merged$pred = predict(model_simple)
predict(model_simple,
type = "class")
df_merged$pred = predict(model_simple,
type = "class")
hist(df_merged$pred)
df_merged$pred
length(df_merged$pred)
nrow(df_merged)
df_merged$pred
df_merged$pred$fit
s = df_merged$pred$fit
length(s)
s
as.numeric(s)
hist(as.numeric(s))
predict(model_simple,
type = "linear.predictor")
s = predict(model_simple,
type = "cum.prob")
s
s$cprob1
hist(df_merged$pred)
hist(df_merged$pred$fit)
s
hist(s)
s$cprob1
hist(s$cprob1)
df_merged$pred = predict(model_simple)
hist(df_merged$pred)
df_merged$pred$fit
df_merged$pred = predict(model_simple)$fit
hist(df_merged$pred)
df_merged$resid2 = df_merged$pred - df_merged$relatedness
df_merged %>%
ggplot(aes(x = resid2,
y = ambiguity_type,
fill = same)) +
geom_density_ridges2(aes(height = ..density..),
color=gray(0.25),
alpha = 0.5,
scale=0.85,
size=.9,
stat="density") +
labs(x = "Residuals (rel ~ ELMo + BERT)",
y = "Ambiguity type") +
theme_minimal()
df_merged$pred = predict(model_simple, type = "class")$fit
df_merged$resid2 = df_merged$pred - df_merged$relatedness
df_merged$pred = as.numeric(predict(model_simple, type = "class")$fit)
df_merged$resid2 = df_merged$pred - df_merged$relatedness
hist(df_merged$pred )
?residuals
df_merged$resid2 = df_merged$pred - df_merged$relatedness
df_merged %>%
ggplot(aes(x = resid2,
y = ambiguity_type,
fill = same)) +
geom_density_ridges2(aes(height = ..density..),
color=gray(0.25),
alpha = 0.5,
scale=0.85,
size=.9,
stat="density") +
labs(x = "Residuals (rel ~ ELMo + BERT)",
y = "Ambiguity type") +
theme_minimal()
df_merged$resid2 = df_merged$relatedness - df_merged$pred
df_merged %>%
ggplot(aes(x = resid2,
y = ambiguity_type,
fill = same)) +
geom_density_ridges2(aes(height = ..density..),
color=gray(0.25),
alpha = 0.5,
scale=0.85,
size=.9,
stat="density") +
labs(x = "Residuals (rel ~ ELMo + BERT)",
y = "Ambiguity type") +
theme_minimal()
setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")
## First, load data about individual items
df_items = read_csv("../../data/items.csv")
nrow(df_items)
df_items_gathered = df_items %>%
pivot_longer(c(list_1, list_2, list_3, list_4),
names_to = "list", values_to = "version")
nrow(df_items_gathered)
## Now, load actual experimental data
df_e1 = read_csv("../../data/processed/translation_e1_processed.csv")
length(unique(df_e1$subject))
# Double-check experimental data
# There should be 120 rows for both tasks combined for each subject, so divide amount per subject by 120
## Two weird issues here: 1) Recall has less than plausibility?; 2) There are 93 subjects, but only 92 and 91 listed?
table(df_e1$task) / 120
### I think the two with "60" were collected 5-21-20? Before we modified the code to also note the "fillers" to be coded by task
df_counts = df_e1 %>%
group_by(task, subject) %>%
summarise(count = n()) %>%
filter(!is.na(task))
range(df_counts$count)
df_items
setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")
## First, load data about individual items
df_items = read_csv("../../data/items.csv")
nrow(df_items)
library(tidyverse)
library(lme4)
setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")
## First, load data about individual items
df_items = read_csv("../../data/items.csv")
nrow(df_items)
df_items_gathered = df_items %>%
pivot_longer(c(list_1, list_2, list_3, list_4),
names_to = "list", values_to = "version")
nrow(df_items_gathered)
df_e1 = read_csv("../../data/processed/translation_e1_processed.csv")
length(unique(df_e1$subject))
# Double-check experimental data
# There should be 120 rows for both tasks combined for each subject, so divide amount per subject by 120
## Two weird issues here: 1) Recall has less than plausibility?; 2) There are 93 subjects, but only 92 and 91 listed?
table(df_e1$task) / 120
### I think the two with "60" were collected 5-21-20? Before we modified the code to also note the "fillers" to be coded by task
df_counts = df_e1 %>%
group_by(task, subject) %>%
summarise(count = n()) %>%
filter(!is.na(task))
range(df_counts$count)
View(df_counts)
df_e1 = df_e1 %>%
filter(subject != "w6od36ss0c") %>%
filter(subject != "5s17pjhk0z")
df_counts = df_e1 %>%
group_by(task, subject) %>%
summarise(count = n()) %>%
filter(!is.na(task))
range(df_counts$count)
setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")
## First, load data about individual items
df_items = read_csv("../../data/items.csv")
nrow(df_items)
df_items_gathered = df_items %>%
pivot_longer(c(list_1, list_2, list_3, list_4),
names_to = "list", values_to = "version")
nrow(df_items_gathered)
## Now, load actual experimental data
df_e1 = read_csv("../../data/processed/translation_e1_processed.csv")
length(unique(df_e1$subject))
setwd("/Users/seantrott/Dropbox/UCSD/Research/Semantics/Translation/translation_study/src/analysis")
## First, load data about individual items
df_items = read_csv("../../data/items.csv")
nrow(df_items)
df_items_gathered = df_items %>%
pivot_longer(c(list_1, list_2, list_3, list_4),
names_to = "list", values_to = "version")
nrow(df_items_gathered)
## Now, load actual experimental data
df_e1 = read_csv("../../data/processed/translation_e1_processed.csv")
length(unique(df_e1$subject))
table(df_e1$task) / 120
df_e1$response = fct_recode(
factor(df_e1$key_press),
"Yes" = "89",
"No" = "78"
)
df_e1 = df_e1 %>%
mutate(correct_response = case_when(
type == "filler" ~ "No",
is.na(type) ~ "Yes"
)) %>%
mutate(correct = correct_response == response)
df_merged = df_e1 %>%
left_join(df_items_gathered, on = c(list, object))
## Label whether or not shape/sentence is listed as matching
df_merged = df_merged %>%
mutate(
match = case_when(
type == "filler" ~ "filler",
version %in% c('aa', 'bb') ~ "yes",
(version %in% c('aa', 'bb')) == FALSE ~ "no"
)
)
df_merged_recall_critical = df_merged %>%
filter(task == "recall") %>%
filter(match != "filler")
df_merged_recall_critical %>%
group_by(match) %>%
summarise(accuracy = mean(correct))
df_merged_recall_critical = df_merged %>%
filter(task == "recall")
df_merged_recall_critical %>%
group_by(match) %>%
summarise(accuracy = mean(correct))
df_merged_recall_critical %>%
ggplot(aes(x = match,
fill = response)) +
geom_bar(stat = "count")
df_merged_recall_critical %>%
ggplot(aes(x = match,
fill = response)) +
geom_bar(stat = "count", position = "dodge")
df_merged_recall_critical %>%
group_by(match) %>%
summarise(accuracy = mean(correct),
count = n())
5460 / 91
2776 / 91
2684 / 91
5460 / 2
df_merged_recall_critical %>%
group_by(match, subject) %>%
summarise(accuracy = mean(correct),
count = n())
View(df_merged_recall_critical)
s = df_merged_recall_critical %>%
group_by(match, subject) %>%
summarise(accuracy = mean(correct),
count = n())
View(s)
df_merged_recall_critical$list
s = df_merged_recall_critical %>%
group_by(match, subject, list) %>%
summarise(accuracy = mean(correct),
count = n())
View(s)
df_items_gathered
View(df_items_gathered)
df_items_gathered = df_items_gathered %>%
mutate(
match = case_when(
version %in% c('aa', 'bb') ~ "yes",
(version %in% c('aa', 'bb')) == FALSE ~ "no"
)
)
View(df_items_gathered)
s = df_items_gathered %>%
group_by(list, match) %>%
summarise(count = n())
View(s)
s = df_merged_recall_critical %>%
group_by(match, object, list) %>%
summarise(accuracy = mean(correct),
count = n())
View(s)
1260 / 60
df_merged_recall_critical = df_merged %>%
filter(task == "plausibility") %>%
filter(match != "filler")
f = df_merged_recall_critical %>%
group_by(match, object, list) %>%
summarise(accuracy = mean(correct),
count = n())
View(f)
f = df_merged_recall_critical %>%
group_by(match, subject, list) %>%
summarise(accuracy = mean(correct),
count = n())
View(f)
table(df_merged_recall_critical$list)
table(df_merged_recall_critical$list) / 60
df_merged$version
factor(df_merged$version)
g = df_merged_recall_critical %>%
group_by(version, subject, list) %>%
summarise(accuracy = mean(correct),
count = n())
View(g)
df_merged_recall_critical = df_merged %>%
filter(task == "plausibility") %>%
filter(match != "filler")
df_merged_recall_critical %>%
group_by(version, subject, list) %>%
summarise(accuracy = mean(correct),
count = n())
View(g)
df_merged_recall_critical %>%
group_by(version, list) %>%
summarise(accuracy = mean(correct),
count = n())
h = df_merged_recall_critical %>%
group_by(version, list) %>%
summarise(accuracy = mean(correct),
count = n())
View(h)
